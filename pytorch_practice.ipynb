{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchの勉強用 \n",
    "- PyTorch公式チュートリアル(https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)を和訳\n",
    "\n",
    "## 目的（なぜやるのか）\n",
    "- PyTorchの基礎的な概念を学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パッケージのimportとPyTorchのバージョンを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.5.0\n",
      "Torchvision Version:  0.6.0\n"
     ]
    }
   ],
   "source": [
    "# PyTorchのバージョン確認\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch公式チュートリアル\n",
    "## PyTorchについて\n",
    "\n",
    "pytorchのコアとなる機能は、主に2つある\n",
    "- n次元テンソル(Tensor)：numpyに似ているが、GPU上でも動作する\n",
    "- 自動微分(Automatic differentiation)：ニューラルネットワークの構築・訓練に用いることができる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テンソル(Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpyのウォームアップ\n",
    "\n",
    "PyTorchを用いる前に、まずはnumpyを用いてネットワークを実装する\n",
    "\n",
    "numpyはn次元の配列オブジェクトと、その配列を操作するための多くの関数を提供している。numpyは計算科学のための汎用的なフレームワークであり、計算グラフ(computation graphs)、ディープラーニング、勾配の概念については特に考えられていない。しかし、2層のニューラルネットワークをランダムなデータに適合させる上で、numpyの演算を使ってネットワークの順伝播・逆伝播のパスを手動で実装することで、numpyでもこれらの概念を実装できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 54384746.05910431\n",
      "1 61313253.855912626\n",
      "2 59508280.3641528\n",
      "3 38322748.59168631\n",
      "4 16196244.449533762\n",
      "5 5841402.904879023\n",
      "6 2850699.193637717\n",
      "7 1925162.8143797358\n",
      "8 1501905.6937964123\n",
      "9 1228933.6185847118\n",
      "10 1024573.13631874\n",
      "11 863622.6656617752\n",
      "12 734249.5826519736\n",
      "13 628541.5677758548\n",
      "14 541238.7188067418\n",
      "15 468533.11262210057\n",
      "16 407557.61121277383\n",
      "17 356165.37801844906\n",
      "18 312495.85045912926\n",
      "19 275213.39513825555\n",
      "20 243180.57273486073\n",
      "21 215545.91751103048\n",
      "22 191598.7752599609\n",
      "23 170785.4188023764\n",
      "24 152631.22350790192\n",
      "25 136727.61170439963\n",
      "26 122761.4496576972\n",
      "27 110478.92072843657\n",
      "28 99637.28630715127\n",
      "29 90037.6135748188\n",
      "30 81518.38416239197\n",
      "31 73945.13313887116\n",
      "32 67190.0102672246\n",
      "33 61151.695713335794\n",
      "34 55746.14070072501\n",
      "35 50896.79661547007\n",
      "36 46542.027985831475\n",
      "37 42623.371920488775\n",
      "38 39090.244845010006\n",
      "39 35899.47144501709\n",
      "40 33013.59836808576\n",
      "41 30398.86510324382\n",
      "42 28025.306404904666\n",
      "43 25867.963559783977\n",
      "44 23904.544956979516\n",
      "45 22116.977833585916\n",
      "46 20486.04205059912\n",
      "47 18995.36515559954\n",
      "48 17631.261710612598\n",
      "49 16382.130145765008\n",
      "50 15236.882449180994\n",
      "51 14185.624904065637\n",
      "52 13218.781943362053\n",
      "53 12329.17351407753\n",
      "54 11509.649360100813\n",
      "55 10753.83072445649\n",
      "56 10056.293544493872\n",
      "57 9411.489655262072\n",
      "58 8815.1585642102\n",
      "59 8263.02375483606\n",
      "60 7751.279643562336\n",
      "61 7276.751927874338\n",
      "62 6836.337642854312\n",
      "63 6426.966536692614\n",
      "64 6046.2504524214255\n",
      "65 5691.884429594897\n",
      "66 5361.820356908933\n",
      "67 5053.937867505279\n",
      "68 4766.720195434512\n",
      "69 4498.57040789731\n",
      "70 4248.201599989379\n",
      "71 4014.337833042551\n",
      "72 3795.431228000069\n",
      "73 3590.3904797375717\n",
      "74 3398.2579413950334\n",
      "75 3218.0639325263387\n",
      "76 3048.9339793416766\n",
      "77 2890.280070218957\n",
      "78 2741.1451526744395\n",
      "79 2600.903587257182\n",
      "80 2468.9067206132604\n",
      "81 2344.6624774001903\n",
      "82 2227.619170330876\n",
      "83 2117.276710070538\n",
      "84 2013.2339187551047\n",
      "85 1915.0725275941447\n",
      "86 1822.396570151543\n",
      "87 1734.869712182499\n",
      "88 1652.161839375156\n",
      "89 1573.9023648236712\n",
      "90 1499.8722917979276\n",
      "91 1429.7948252236488\n",
      "92 1363.4424378832118\n",
      "93 1300.5853265363005\n",
      "94 1241.0304867314799\n",
      "95 1184.532410903532\n",
      "96 1130.9953840357505\n",
      "97 1080.4976639192078\n",
      "98 1032.5485095915815\n",
      "99 987.0116715625463\n",
      "100 943.7622469729506\n",
      "101 902.650277025178\n",
      "102 863.5468296356643\n",
      "103 826.3564711701722\n",
      "104 790.9733946559204\n",
      "105 757.2946339600323\n",
      "106 725.235559557518\n",
      "107 694.7111394718835\n",
      "108 665.6293956923662\n",
      "109 637.9076763672106\n",
      "110 611.4790581133566\n",
      "111 586.2728508816257\n",
      "112 562.2353243181444\n",
      "113 539.2914604264022\n",
      "114 517.3858976289429\n",
      "115 496.46743605788373\n",
      "116 476.49000139969564\n",
      "117 457.40020017094656\n",
      "118 439.1662246373962\n",
      "119 421.73048350919066\n",
      "120 405.0548611717846\n",
      "121 389.1073071887755\n",
      "122 373.8529209897751\n",
      "123 359.25610618097005\n",
      "124 345.31243760429504\n",
      "125 331.9648873487083\n",
      "126 319.18267012700016\n",
      "127 306.9372267983029\n",
      "128 295.2051554664555\n",
      "129 283.96616527387414\n",
      "130 273.1943662578166\n",
      "131 262.86451219668294\n",
      "132 252.95895191380657\n",
      "133 243.458981995669\n",
      "134 234.34696106059917\n",
      "135 225.60465446256313\n",
      "136 217.21285743092713\n",
      "137 209.15832698788122\n",
      "138 201.427425354717\n",
      "139 194.00407502083783\n",
      "140 186.87736690406257\n",
      "141 180.03111832047207\n",
      "142 173.45309566745738\n",
      "143 167.1353859441777\n",
      "144 161.06302806189925\n",
      "145 155.22854379089137\n",
      "146 149.61853091070327\n",
      "147 144.22602648741895\n",
      "148 139.04023311881338\n",
      "149 134.0542705999839\n",
      "150 129.2598451835399\n",
      "151 124.64796457384111\n",
      "152 120.21038491575044\n",
      "153 115.94016129583699\n",
      "154 111.8325145873173\n",
      "155 107.8784981691167\n",
      "156 104.07242244655679\n",
      "157 100.40952570647977\n",
      "158 96.88302522015456\n",
      "159 93.4877778104068\n",
      "160 90.21760036783834\n",
      "161 87.0681467261147\n",
      "162 84.03502834217443\n",
      "163 81.11416580414935\n",
      "164 78.30022942914829\n",
      "165 75.58797472691114\n",
      "166 72.97486289225833\n",
      "167 70.45724262894706\n",
      "168 68.03071901419165\n",
      "169 65.69176193132064\n",
      "170 63.43754202186338\n",
      "171 61.2641088307866\n",
      "172 59.16914076349535\n",
      "173 57.14878366831293\n",
      "174 55.20086425759027\n",
      "175 53.32257259249798\n",
      "176 51.511133901611466\n",
      "177 49.76369833308984\n",
      "178 48.07802042095568\n",
      "179 46.45200425632483\n",
      "180 44.88511820577786\n",
      "181 43.37174522110002\n",
      "182 41.911307481876946\n",
      "183 40.50229374327799\n",
      "184 39.14296502024071\n",
      "185 37.83089724891205\n",
      "186 36.56438553040109\n",
      "187 35.341844909188836\n",
      "188 34.161945825078504\n",
      "189 33.02314183092352\n",
      "190 31.923381507095424\n",
      "191 30.861579524471253\n",
      "192 29.836659427536215\n",
      "193 28.847078967710505\n",
      "194 27.891152836016968\n",
      "195 26.968171638813203\n",
      "196 26.076939721773822\n",
      "197 25.216171128746815\n",
      "198 24.384672063248694\n",
      "199 23.581433541883282\n",
      "200 22.80571082216855\n",
      "201 22.05636769981029\n",
      "202 21.33228932397254\n",
      "203 20.632679644752457\n",
      "204 19.957037557481385\n",
      "205 19.304039130615507\n",
      "206 18.67306768482127\n",
      "207 18.063389896850442\n",
      "208 17.47428188718444\n",
      "209 16.9048126904765\n",
      "210 16.35449863813288\n",
      "211 15.822625016622794\n",
      "212 15.308610403241975\n",
      "213 14.81172334828698\n",
      "214 14.331405837527072\n",
      "215 13.867063525579702\n",
      "216 13.418310390615826\n",
      "217 12.984425877409542\n",
      "218 12.56485793519149\n",
      "219 12.159326044780407\n",
      "220 11.767209066265679\n",
      "221 11.387976978070053\n",
      "222 11.021351543556808\n",
      "223 10.666846872309462\n",
      "224 10.323984953162649\n",
      "225 9.992453921865382\n",
      "226 9.671789672189599\n",
      "227 9.361775081656443\n",
      "228 9.061859336801287\n",
      "229 8.771747922409663\n",
      "230 8.491214646027757\n",
      "231 8.21987790199882\n",
      "232 7.957614979038862\n",
      "233 7.703912651379126\n",
      "234 7.458551302904176\n",
      "235 7.221116744253839\n",
      "236 6.991441614437565\n",
      "237 6.769244805918681\n",
      "238 6.554290439066742\n",
      "239 6.346271699104622\n",
      "240 6.14514615427539\n",
      "241 5.950514455298343\n",
      "242 5.762057233408093\n",
      "243 5.579688713746329\n",
      "244 5.403219728197403\n",
      "245 5.2324721139997346\n",
      "246 5.0671867165608475\n",
      "247 4.907246592020584\n",
      "248 4.752481006122977\n",
      "249 4.602655966689671\n",
      "250 4.457653096642777\n",
      "251 4.31730241886701\n",
      "252 4.18147235976598\n",
      "253 4.0499791160536995\n",
      "254 3.9226872030492426\n",
      "255 3.7995041463333847\n",
      "256 3.6802335840396063\n",
      "257 3.564773773996716\n",
      "258 3.45302305858633\n",
      "259 3.3448245571296153\n",
      "260 3.240070719727846\n",
      "261 3.138653037097663\n",
      "262 3.040478272044843\n",
      "263 2.9454138985413914\n",
      "264 2.8533695342200573\n",
      "265 2.764266590338324\n",
      "266 2.67797907375745\n",
      "267 2.5944235185412836\n",
      "268 2.513536430474324\n",
      "269 2.4352032228580907\n",
      "270 2.3593435924173267\n",
      "271 2.2858841214310783\n",
      "272 2.2147575224701783\n",
      "273 2.14587086384053\n",
      "274 2.079154421611957\n",
      "275 2.01455648091686\n",
      "276 1.951988127073209\n",
      "277 1.8913878102271202\n",
      "278 1.8327106844368442\n",
      "279 1.775872200540547\n",
      "280 1.7208189475419793\n",
      "281 1.6674992919571328\n",
      "282 1.6158592909806058\n",
      "283 1.5658352240862479\n",
      "284 1.5173792325689175\n",
      "285 1.470454952238668\n",
      "286 1.4249908666609663\n",
      "287 1.3809544762730481\n",
      "288 1.3383056688669865\n",
      "289 1.2969823891782357\n",
      "290 1.2569518475479522\n",
      "291 1.2181757092299637\n",
      "292 1.1806121920190473\n",
      "293 1.1442172568349462\n",
      "294 1.1089623358750635\n",
      "295 1.074821477391852\n",
      "296 1.0417661329943584\n",
      "297 1.0097067227910452\n",
      "298 0.9786473811244805\n",
      "299 0.948551603508051\n",
      "300 0.91939149248493\n",
      "301 0.891143839176687\n",
      "302 0.8637693032921713\n",
      "303 0.8372460925003167\n",
      "304 0.8115511044972905\n",
      "305 0.7866492494104522\n",
      "306 0.76252059000298\n",
      "307 0.7391455024538169\n",
      "308 0.7164916731532767\n",
      "309 0.6945380560498071\n",
      "310 0.6732684159826022\n",
      "311 0.6526538918895968\n",
      "312 0.6326780897714335\n",
      "313 0.6133218780970673\n",
      "314 0.5945631089526211\n",
      "315 0.5763825959819454\n",
      "316 0.5587675273146027\n",
      "317 0.5416943772625029\n",
      "318 0.5251476164271016\n",
      "319 0.509112866828239\n",
      "320 0.49357217133882725\n",
      "321 0.47850944963998343\n",
      "322 0.4639130834901554\n",
      "323 0.44976513049054445\n",
      "324 0.4360524494648079\n",
      "325 0.42276319057756806\n",
      "326 0.4098833076512482\n",
      "327 0.39739806034193204\n",
      "328 0.38529818811591776\n",
      "329 0.37356891326713526\n",
      "330 0.3622000267974669\n",
      "331 0.3511811725854235\n",
      "332 0.34050016985512566\n",
      "333 0.33014633504714574\n",
      "334 0.32011162563842793\n",
      "335 0.31038403842174433\n",
      "336 0.30095400755541\n",
      "337 0.2918142339531845\n",
      "338 0.2829534436763157\n",
      "339 0.2743639193232673\n",
      "340 0.2660384505071066\n",
      "341 0.2579668547244015\n",
      "342 0.2501419495612792\n",
      "343 0.2425578197789544\n",
      "344 0.23520477376570964\n",
      "345 0.22807611927046356\n",
      "346 0.22116616983847528\n",
      "347 0.21446632751507325\n",
      "348 0.20797131952629855\n",
      "349 0.20167496596604995\n",
      "350 0.19557012837850285\n",
      "351 0.18966075739109883\n",
      "352 0.18392399120269176\n",
      "353 0.17836078878287764\n",
      "354 0.17296723091639707\n",
      "355 0.16773815703745065\n",
      "356 0.16266788886190753\n",
      "357 0.15775246472080418\n",
      "358 0.15298655403957812\n",
      "359 0.14836536957012034\n",
      "360 0.143885538668766\n",
      "361 0.13954146492150196\n",
      "362 0.13532939976511732\n",
      "363 0.13124589660677796\n",
      "364 0.12728586994665267\n",
      "365 0.12344624378644718\n",
      "366 0.11972363179602415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367 0.11611362090064145\n",
      "368 0.11261335387359989\n",
      "369 0.10921944046574383\n",
      "370 0.1059282741697617\n",
      "371 0.10273726164627295\n",
      "372 0.09964278877129149\n",
      "373 0.09664202201881575\n",
      "374 0.09373255046551235\n",
      "375 0.0909110076641462\n",
      "376 0.08817489634581135\n",
      "377 0.08552192121469035\n",
      "378 0.08294891718979958\n",
      "379 0.0804539840940541\n",
      "380 0.07803469259521438\n",
      "381 0.0756883001162931\n",
      "382 0.07341309535645071\n",
      "383 0.0712066213433411\n",
      "384 0.06906675631846951\n",
      "385 0.0669918407618617\n",
      "386 0.06497936617572554\n",
      "387 0.06302777848128144\n",
      "388 0.061135311531690245\n",
      "389 0.05929979623173878\n",
      "390 0.057519870715624546\n",
      "391 0.0557935722597942\n",
      "392 0.054119274710928415\n",
      "393 0.05249570237043546\n",
      "394 0.0509209323117797\n",
      "395 0.04939368438771351\n",
      "396 0.047912619958565154\n",
      "397 0.046476076826945295\n",
      "398 0.04508289502388513\n",
      "399 0.04373167816329767\n",
      "400 0.04242109865757906\n",
      "401 0.041150162463895655\n",
      "402 0.03991735105393476\n",
      "403 0.03872170796232814\n",
      "404 0.03756215233651696\n",
      "405 0.03643779992454874\n",
      "406 0.03534852877831607\n",
      "407 0.03429038057138928\n",
      "408 0.033264036542582774\n",
      "409 0.03226866786013845\n",
      "410 0.03130310997354653\n",
      "411 0.030366645245162648\n",
      "412 0.029458335831187926\n",
      "413 0.028577264538637924\n",
      "414 0.027722780401266654\n",
      "415 0.026893863583462546\n",
      "416 0.02608986579191052\n",
      "417 0.025310076711788258\n",
      "418 0.024553610484696592\n",
      "419 0.02381992083269507\n",
      "420 0.023108218512448916\n",
      "421 0.022417858448042425\n",
      "422 0.02174828437737831\n",
      "423 0.021098711320980283\n",
      "424 0.020468674989904914\n",
      "425 0.019857513031434206\n",
      "426 0.019264650356839354\n",
      "427 0.018689626871985464\n",
      "428 0.018131773339848833\n",
      "429 0.017590677619177368\n",
      "430 0.017065814984635948\n",
      "431 0.0165566308441315\n",
      "432 0.016062752818095445\n",
      "433 0.015583603749643042\n",
      "434 0.015118836098021055\n",
      "435 0.014667984873833993\n",
      "436 0.014230603412268669\n",
      "437 0.013806365691146802\n",
      "438 0.01339477663412494\n",
      "439 0.012995532071718736\n",
      "440 0.012608223443820616\n",
      "441 0.01223248028597981\n",
      "442 0.011868025055782473\n",
      "443 0.01151442730361761\n",
      "444 0.011171419655432495\n",
      "445 0.010838662139987385\n",
      "446 0.01051584104772975\n",
      "447 0.010202701468876713\n",
      "448 0.009898882520261047\n",
      "449 0.009604164307025011\n",
      "450 0.009318241281806517\n",
      "451 0.0090408557986345\n",
      "452 0.008771774786849368\n",
      "453 0.008510704910480296\n",
      "454 0.008257456040899598\n",
      "455 0.008011760255637115\n",
      "456 0.007773391471327386\n",
      "457 0.0075421478576892485\n",
      "458 0.007317788499073995\n",
      "459 0.007100147368275683\n",
      "460 0.006888977996390901\n",
      "461 0.006684415022947097\n",
      "462 0.006485749314785836\n",
      "463 0.006292914408710833\n",
      "464 0.006105842117047176\n",
      "465 0.005924330825325407\n",
      "466 0.005748248572399084\n",
      "467 0.005577408869016498\n",
      "468 0.005411659468497249\n",
      "469 0.005250855692336063\n",
      "470 0.00509483266951513\n",
      "471 0.004943478684536517\n",
      "472 0.004796614957283937\n",
      "473 0.004654136990848436\n",
      "474 0.004515898818837621\n",
      "475 0.004381776919175679\n",
      "476 0.004251656322458447\n",
      "477 0.004125400551176312\n",
      "478 0.004002920079819044\n",
      "479 0.0038840736792343142\n",
      "480 0.003768769706736989\n",
      "481 0.00365689555691282\n",
      "482 0.0035483499482652616\n",
      "483 0.0034430397952709972\n",
      "484 0.0033408556975940244\n",
      "485 0.0032417217653797226\n",
      "486 0.0031455279165118095\n",
      "487 0.003052202742111094\n",
      "488 0.0029616492805794896\n",
      "489 0.002873790623099539\n",
      "490 0.0027885472923562414\n",
      "491 0.0027058321815915587\n",
      "492 0.002625584520390004\n",
      "493 0.002547714555370708\n",
      "494 0.0024721659047980144\n",
      "495 0.0023988598388179133\n",
      "496 0.0023277331570460045\n",
      "497 0.0022587209173224\n",
      "498 0.0021917568911456378\n",
      "499 0.0021267871357519034\n"
     ]
    }
   ],
   "source": [
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 重みをランダムな値で初期化\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "# 学習率の設定\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # 順伝播：yの予測値を計算\n",
    "    h = x.dot(w1) # 入力に重みw1をかけて隠れ層に渡す\n",
    "    h_relu = np.maximum(h, 0)# reLU関数で活性化\n",
    "    y_pred = h_relu.dot(w2) # 重みw2をかけてyの予測値を算出\n",
    "\n",
    "    # 損失の計算\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 逆伝播：損失を元に重みw1, w2の勾配を計算\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 重みの更新\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch：Tensor\n",
    "\n",
    "numpyは素晴らしいフレームワークであるが、GPUを用いて数値計算を高速化することはできない。最近のディープニューラルネットワークでは、GPUで50倍以上の高速化が可能な場合が多いため、残念ながらnumpyだけでは不十分である\n",
    "\n",
    "ここで、PyTorchの最も基本的な概念であるテンソル(Tensor)を紹介する。PyTorchのテンソルは、概念的にはnumpy配列と同じである。しかし、numpy配列に加えて計算グラフや勾配の追跡も可能である他、GPUを利用して数値計算を高速化することが可能である。GPU上でPyTorch Tensorを実行するには、新しいデータ型にキャストする必要がある\n",
    "\n",
    "以下では、上記のnumpyの例と同じようにランダムなデータに2層のネットワークをフィットさせる際、PyTorch Tensorを用いている。ここで、ネットワークの順伝播・逆伝播のパスも同様に手動で実装する必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 308.6850891113281\n",
      "199 0.7617379426956177\n",
      "299 0.002684763167053461\n",
      "399 8.705579239176586e-05\n",
      "499 2.1205840312177315e-05\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # GPUを用いる場合はこの文をアンコメント\n",
    "\n",
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 重みをランダムな値で初期化\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 学習率の設定\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 順伝播：yの予測値を計算\n",
    "    h = x.mm(w1) # mm：2次元の行列同士の積を計算する関数\n",
    "    h_relu = h.clamp(min=0) # clamp(input, min, max)：inputのすべての要素について、[min,max]の範囲外の値をそれぞれmin,maxに変換する関数\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 損失の計算\n",
    "    loss = (y_pred - y).pow(2).sum().item() # pow(exponent):exponentの値で累乗する関数\n",
    "    if t % 100 == 99: # 100ステップごとに損失をプリント\n",
    "        print(t, loss)\n",
    "\n",
    "    # 逆伝播：損失を元に重みw1, w2の勾配を計算\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 勾配降下を用いて重みの更新\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd\n",
    "\n",
    "### PyTorch：Tensorとautograd\n",
    "上記の例ではニューラルネットワークの順伝播・逆伝播のパスを手動で実装していた。しかし、大規模で複雑なネットワークでは同じことをしようとすると非常に手間がかかってしまう。\n",
    "\n",
    "ここで、PyTorchでは自動微分(automatic differentiation)を用いてニューラルネットワークの逆伝播の計算を自動化することができる。PyTorchのautogradパッケージがこの機能を持つ。autogradを用いると、ネットワークの順伝播パスにて計算グラフが定義される。グラフのノードはテンソルで、エッジは入力テンソルから出力テンソルを生成する関数である。このグラフに対して誤差逆伝播を行うことで、勾配を簡単に計算することができる\n",
    "\n",
    "これは一見複雑に聞こえるが、実際には非常に簡単に用いることができる。各テンソルは計算グラフのノードを表し、あるテンソルxがx.require_grad=Trueのテンソルであれば、x.gradはスカラー値に対するxの勾配を保持する別のテンソルである\n",
    "\n",
    "以下では、PyTorch Tensorとautogradを用いて2層ネットワークを実装している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 621.9561157226562\n",
      "199 5.948866367340088\n",
      "299 0.08107059448957443\n",
      "399 0.0014935587532818317\n",
      "499 0.00011179103603353724\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPUを用いる場合はこの文をアンコメント\n",
    "\n",
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "# requires_grad=Falseに設定することで、逆伝播の際そのテンソルに関する勾配は計算しなくなる\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 重みをランダムな値で初期化\n",
    "# requires_grad=Trueに設定することで、逆伝播の際にそのテンソルに関する勾配を計算するようになる\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 順伝播：これは先程のTensorsを用いた順伝播パスの計算に用いたものと全く同じ\n",
    "    # しかし、逆伝播を手作業で実装していないので、中間値を保持する必要がない\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # テンソルの演算を用いて損失を計算\n",
    "    # この時、損失はテンソルのshape (1,)に保持されている\n",
    "    # loss.item()はlossに保持しているスカラー値を取得する関数\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd を使用して逆伝播を計算する。この呼び出しでは、requires_grad=True のすべてのテンソルについて損失の勾配を計算している\n",
    "    #この呼び出しの後、w1.gradとw2.gradはそれぞれw1とw2に対する損失の勾配を保持するテンソルとなる\n",
    "    loss.backward()\n",
    "\n",
    "    # 勾配降下を用いて手動で重みの更新を行い torch.no_grad()でラップする\n",
    "    # 重みのテンソルはrequires_grad=Trueであるため、手動で追跡する必要はない\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 重みを更新した後、手動で勾配をゼロにする\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch：新しいautograd関数の定義\n",
    "それぞれの原始的なautograd演算子は、実際にはテンソルで動作する2つの関数である。forward関数は入力テンソルから出力テンソルを計算する。backward関数はスカラー値に対する出力テンソルの勾配を受け取り、同じスカラー値に対応する入力テンソルの勾配を計算する\n",
    "\n",
    "PyTorchでは、torch.autograd.Functionのサブクラスを定義し、forward関数とbackward関数を実装することで、簡単に独自のautograd演算子を定義することができる。インスタンスを作成して関数のように呼び出し、入力データを含むテンソルを渡すことで、autograd演算子を使用することができる\n",
    "\n",
    "以下の例では、ReLU非線形性を実行するための独自のautograd関数を定義し、それを用いて2層のネットワークを実装している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 259.4796447753906\n",
      "199 0.5658110976219177\n",
      "299 0.0022444366477429867\n",
      "399 9.172491263598204e-05\n",
      "499 2.313937511644326e-05\n"
     ]
    }
   ],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Functionをサブクラス化し、\n",
    "    テンソル上で動作する順伝播パスと逆伝播パスを実装することで、\n",
    "    独自のautograd Functionsを実装できる\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        順伝播パスでは、入力を含むテンソルを受け取り、出力を含むテンソルを返す\n",
    "        ctxは、逆伝播の計算のための情報を格納するために用いることのできるコンテキストオブジェクトである\n",
    "        ctx.save_for_backwardメソッドを使用することで、任意のオブジェクトをキャッシュして逆伝播パスにて用いることができる\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        逆伝播パスでは、出力に対する損失の勾配を受け取り、\n",
    "        そこから入力に対する損失の勾配を計算する必要がある\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPUを用いる場合はこの文をアンコメント\n",
    "\n",
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 重みをランダムな値で初期化\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 関数を適用するには、Function.applyメソッドを用いる。今回はこれを'relu'とエイリアスする\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # 順伝播：カスタムしたautograd演算を用いてReLUを計算し、それを用いて予測されたyを計算する\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 損失を計算し、print\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd を使用して逆伝播を計算\n",
    "    loss.backward()\n",
    "\n",
    "    # 勾配降下を用いて重みの更新\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 重みを更新した後、手動で勾配をゼロにする\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nnモジュール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch：nn\n",
    "計算グラフとautogradは複雑な演算子を定義し、自動的に導関数を得るうえで非常に強力なパラダイムである。しかしながら、大規模なニューラルネットワークの場合、生のautogradは少し低レベルすぎるかもしれない\n",
    "\n",
    "ニューラルネットワークを構築する際には、計算を層に配置することをよく考えるが、その中には学習可能なパラメータがあり、学習中に最適化される\n",
    "\n",
    "TensorFlowでは、Keras, TensorFlow-Slim, TFLearnなどのパッケージが、ニューラルネットワークを構築するうえで便利な、生の計算グラフ上の高レベルの抽象化を提供している\n",
    "\n",
    "PyTorchでは、nnパッケージがこれと同じ目的を果たしている。nnパッケージは、ニューラルネットワーク層とほぼ同等のモジュールを定義している。モジュール入力テンソルを受け取り、出力テンソルを計算するが、学習可能パラメータを含むテンソルのような内部状態を保持することもできる。また、nnパッケージは、ニューラルネットワークを学習する際に一般的に用いられる有用な損失関数のセットも定義している\n",
    "\n",
    "以下の例では、nnパッケージを用いて2層ネットワークを実装している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.0699803829193115\n",
      "199 0.04423416778445244\n",
      "299 0.0021148109808564186\n",
      "399 0.0001349154335912317\n",
      "499 9.769574717211071e-06\n"
     ]
    }
   ],
   "source": [
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn パッケージを使用して、モデルを層のシーケンスとして定義する\n",
    "# nn.Sequentialは、他のモジュールを含み、それらを順番に適用して出力を生成するモジュールである\n",
    "# 各Linearモジュールは、線形関数を使って入力から出力を計算し、重みとバイアスのための内部テンソルを保持する\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# nnパッケージには、一般的な損失関数の定義も含まれている。今回は損失関数として平均二乗誤差(MSE)を使用\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 順伝播：xをモデルに渡すことでyの予測値を計算する\n",
    "    # モジュールオブジェクトは__call__演算子をオーバーライドするので、関数のように呼び出すことができる\n",
    "    # 入力データのテンソルをモジュールに渡すと出力データのテンソルが生成される\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失を計算してprintする\n",
    "    # loss_fnにyの予測値のテンソルと真値のテンソルを渡すことで、損失を含むテンソルが出力される\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 逆伝播を実行する前に勾配をゼロにする\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 逆伝播：モデルのすべての学習可能パラメータに対する損失の勾配を計算する\n",
    "    # 内部では、各モジュールのパラメータはrequires_grad=Trueでテンソルに格納されているので、\n",
    "    # この呼び出しではモデル内のすべての学習可能なパラメータの勾配を計算する\n",
    "    loss.backward()\n",
    "\n",
    "    # 勾配降下を用いて重みを更新する。各パラメータはテンソルであるため、先程と同じくその勾配にアクセスできる\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim\n",
    "ここまでは、学習可能なパラメータを保持しているテンソルを手動で変異させることでモデルの重みを更新していた（autogradによる過去の値の追跡を避けるため、torch.no_grad()や.dataを使用していた）。これは確率的勾配降下のような単純な最適化アルゴリズムでは大きな負担とはならないが、実際にはAdaGrad, RMSProp, Adamなどのより高度な最適化アルゴリズムを用いてニューラルネットワークを訓練することがよくある\n",
    "\n",
    "PyTorchのoptimパッケージでは、最適化アルゴリズムの概念を抽象化し、一般的に用いられている最適化アルゴリズムの実装を提供している\n",
    "\n",
    "以下の例では、一つ上と同様にnnパッケージを用いてモデルを定義しているが、optimパッケージが提供するAdamアルゴリズムを用いてモデルを最適化している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 32.187049865722656\n",
      "199 0.47134101390838623\n",
      "299 0.004644058644771576\n",
      "399 1.6544943719054572e-05\n",
      "499 2.2352482531573514e-08\n"
     ]
    }
   ],
   "source": [
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nnパッケージを用いてモデルと損失関数を定義\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optimパッケージを使用して、モデルの重みを更新する最適化アルゴリズムを定義する\n",
    "# ここではAdamを使用する。optimパッケージには他にも多くの最適化アルゴリズムが含まれている\n",
    "# Adam コンストラクタの最初の引数は、最適化アルゴリズムがどのテンソルを更新するかを指定する\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # 順伝播：xをモデルに渡すことでyの予測値を計算する\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失を計算してprint\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 逆伝播を行う前に、optimizerオブジェクトを用いて更新する変数（モデルの学習可能な重み）の勾配をすべてゼロにする\n",
    "    # これは、デフォルトでは.backward()が呼び出される度に勾配がバッファに蓄積される（つまり上書きされない）ためである\n",
    "    # 詳細はtorch.autograd.backwardの解説を参照\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 逆伝播：モデルパラメータに対する損失の勾配を計算する\n",
    "    loss.backward()\n",
    "\n",
    "    # optimizerのステップ関数を呼び出すと、そのパラメータが更新される\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch：nnモデルのカスタム\n",
    "\n",
    "時には、既存のモジュールのシーケンスよりも複雑なモデルを指定したいこともあるだろう。このような場合には、nn.Moduleをサブクラス化し、入力テンソルを受け取り、テンソル演算のための他のモジュールや他のautogradを生成する順伝播を定義することで、独自のモジュールを定義することができる\n",
    "\n",
    "以下の例では、2層のネットワークをカスタムモジュールのサブクラスとして実装している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.3383655548095703\n",
      "199 0.04339797794818878\n",
      "299 0.0017115985974669456\n",
      "399 9.253511962015182e-05\n",
      "499 6.024424692441244e-06\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        このコンストラクタでは、2つのnn.Linearモジュールをインスタンス化し、メンバ変数として割り当てる\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        順伝播関数では、入力データのテンソルから出力データのテンソルを返さなければならない\n",
    "        ここで、コンストラクタで定義されたモジュールや、テンソルに対する任意の演算子を用いることができる\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 上記で定義したクラスをインスタンス化してモデルを構築する\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 損失関数と最適化アルゴリズムを定義する\n",
    "# SGDコンストラクタのmodel.parameters() の呼び出しには、モデルのメンバ変数である\n",
    "# 2つのnn.Linearモジュールの学習可能なパラメータが含まれる\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 順伝播：xをモデルに渡すことでyの予測値を計算する\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失を計算してprint\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 勾配をゼロにし、逆伝播を実行し、重みを更新する\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch：フロー制御＋重み共有\n",
    "\n",
    "動的モデルと重み共有の例として、非常に奇妙なモデルを実装する\n",
    "完全に接続されたReLUネットワークは、各順伝播パスにて1から4の間の乱数を選択し、その数ぶんの隠れ層を用い、最内層の隠れ層を計算するために同じ重みを複数回再利用する\n",
    "\n",
    "このモデルでは、ループを実装する上で通常のPythonのフロー制御を用いることができ、順伝播パスを定義する際に同じモジュールを複数回再利用することで、最内層での重み共有を実装することができる\n",
    "\n",
    "このモデルは以下のようにモジュールのサブクラスとして簡単に実装ができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 47.88127136230469\n",
      "199 2.7229580879211426\n",
      "299 2.4152848720550537\n",
      "399 0.48642584681510925\n",
      "499 0.11632964015007019\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        コンストラクタでは、順伝播パスで使用する 3 つの nn.Linearインスタンスを構築する\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        モデルの順伝播パスでは、0,1,2,3のいずれかをランダムに選択し、\n",
    "        middle_linearモジュールを何度も再利用して、隠れ層の表現を計算する\n",
    "        \n",
    "        各順伝播パスは動的な計算グラフを構築するため、モデルの順伝播パスを定義する際に\n",
    "        ループや条件文のような通常のPythonのフロー制御演算子を使用することができる\n",
    "        \n",
    "        ここでは、計算グラフを定義する際に同じモジュールを何度も再利用しても全く問題ないことがわかる\n",
    "        これは各モジュールを一度しか使用できなかったLua Torchからの大きな改善である\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# N：バッチ数 \n",
    "# D_in：入力層の次元数\n",
    "# H：隠れ層の次元数\n",
    "# D_out：出力層の次元数\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# ランダムな入力・出力データの作成\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 上記で定義したクラスをインスタンス化してモデルを構築する\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 損失関数と最適化アルゴリズムを構築する\n",
    "# この奇妙なモデルをバニラの確率的勾配降下法で学習するのは難しいため、今回はmomentumを用いる\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # 順伝播：xをモデルに渡すことでyの予測値を計算する\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 損失を計算してprint\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 勾配をゼロにし、逆伝播を実行し、重みを更新する\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8699, 0.3236, 0.9675],\n",
       "        [0.2016, 0.1074, 0.6263],\n",
       "        [0.4429, 0.7860, 0.8440],\n",
       "        [0.0281, 0.0070, 0.1349],\n",
       "        [0.9484, 0.4282, 0.2927]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5665e-01, 1.0469e-01, 9.3603e-01],\n",
       "        [4.0656e-02, 1.1537e-02, 3.9228e-01],\n",
       "        [1.9616e-01, 6.1779e-01, 7.1225e-01],\n",
       "        [7.9006e-04, 4.8473e-05, 1.8189e-02],\n",
       "        [8.9940e-01, 1.8334e-01, 8.5687e-02]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4672, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0667, 0.0667, 0.0667],\n",
       "        [0.0667, 0.0667, 0.0667],\n",
       "        [0.0667, 0.0667, 0.0667],\n",
       "        [0.0667, 0.0667, 0.0667],\n",
       "        [0.0667, 0.0667, 0.0667]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0208, -0.3717,  0.0967], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  -85.2232, -1522.4877,   396.1404], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2 \n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.0960e+02, 4.0960e+03, 4.0960e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
